{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ikochoy/Mix-MatchImageCaptioning/blob/main/CSC413_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT5OPyWXJ_lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e305aa9-a3f9-45ab-fc01-7b907aec56d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# linear algebra\n",
        "import numpy as np  \n",
        "# data processing, CSV file I / O (e.g. pd.read_csv)\n",
        "import pandas as pd  \n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt  # for plotting data\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9ZOmgQRWWmg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifDOmbIgJqWT"
      },
      "outputs": [],
      "source": [
        "CROPPED_WIDTH = 299\n",
        "CROPPED_HEIGHT = 299"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iCT5qq1li1P"
      },
      "source": [
        "# DataSets Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rat0IrHpriDX"
      },
      "outputs": [],
      "source": [
        "class Image:\n",
        "  def __init__(self, image_id, annotations=[], file_name=None):\n",
        "    self.image_id = image_id\n",
        "    self.annotations = annotations\n",
        "    self.file_name = file_name\n",
        "  \n",
        "  def add_annotation(self, annotation):\n",
        "    self.annotations.append(annotation)\n",
        "\n",
        "  def get_annotations(self):\n",
        "    return self.annotations\n",
        "\n",
        "  def __str__(self):\n",
        "    idx = f\"{self.image_id}\\n\" \n",
        "    annotations = \"\\n\".join(self.annotations)\n",
        "    file_url = f\"{self.file_name}\"\n",
        "    return idx + annotations + file_url\n",
        "\n",
        "class ImageSet:\n",
        "  def __init__(self, images={}):\n",
        "    # Dictionary of images\n",
        "    images\n",
        "    self.images = images\n",
        "  \n",
        "  def add_image(self,image_id, image):\n",
        "    self.images[image_id] = image\n",
        "  \n",
        "  def get_image_by_id(self, image_id):\n",
        "    return self.images[image_id]\n",
        "  \n",
        "  def get_image_list(self):\n",
        "\n",
        "\n",
        "\n",
        "class DataSet:\n",
        "  def __init__(self, images):\n",
        "    self.train = ImageSet()\n",
        "    self.validation = ImageSet()\n",
        "  \n",
        "  def add_training_image(self, image):\n",
        "    self.train.add_image(image)\n",
        "  \n",
        "  def add_validation_image(self, image):\n",
        "    self.validation.add_image(self)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Images From Urls"
      ],
      "metadata": {
        "id": "TpHcbHKba3t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_image(path, image_url, image_id):\n",
        "  response = requests.get(image_url)\n",
        "  img = response.content\n",
        "  if f'{image_id}.jpg' not in os.listdir(path):\n",
        "      with open(path + \"/\" + f'{image_id}' + '.jpg', 'wb') as handler:\n",
        "        handler.write(img)\n",
        "\n",
        "\n",
        "def load_images_to_dir(path, imageset):\n",
        "  # load images concurrently\n",
        "  with concurrent.futures.ThreadPoolExecutor(\n",
        "        max_workers=8\n",
        "    ) as executor:\n",
        "        future_to_url = {\n",
        "            executor.submit(download_image, path, image.file_name, image_id): image.file_name\n",
        "            for image_id, image in imageset.images.items()\n",
        "        }\n",
        "        for future in tqdm(concurrent.futures.as_completed(\n",
        "            future_to_url\n",
        "        )):\n",
        "            url = future_to_url[future]\n",
        "            try:\n",
        "                future.result()\n",
        "            except Exception as exc:\n",
        "                print(\n",
        "                    \"%r generated an exception: %s\" % (url, exc)\n",
        "                )"
      ],
      "metadata": {
        "id": "EMT-pv91a7ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvtdLSBGnc2q"
      },
      "source": [
        "## Microsoft COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7pSwt8ej5Wc"
      },
      "outputs": [],
      "source": [
        "#!unzip /content/drive/MyDrive/Important_Undergrad_Courses/csc413/captions_train-val2014.zip -d /content/drive/MyDrive/Important_Undergrad_Courses/csc413"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnGKhgMfQWgl"
      },
      "outputs": [],
      "source": [
        "! mkdir coco_train\n",
        "! mkdir coco_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diaePxzaWSOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "9c446630-3f5a-42a6-ff2a-aead190b106a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0811546f4513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Important_Undergrad_Courses/csc413/annotations/captions_val2014.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Important_Undergrad_Courses/csc413/annotations/captions_train2014.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Important_Undergrad_Courses/csc413/annotations/captions_val2014.json'"
          ]
        }
      ],
      "source": [
        "valid = json.load(open('/content/drive/MyDrive/Important_Undergrad_Courses/csc413/annotations/captions_val2014.json', 'r'))\n",
        "train = json.load(open('/content/drive/MyDrive/Important_Undergrad_Courses/csc413/annotations/captions_train2014.json', 'r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNGpPxHBefDg"
      },
      "outputs": [],
      "source": [
        "print(train[\"images\"][0])\n",
        "print(train[\"annotations\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zxxlqa2glPf"
      },
      "outputs": [],
      "source": [
        "def get_intermediate_dict(data):\n",
        "  result = {}\n",
        "  for annotation in data[\"annotations\"]:\n",
        "    image_id = annotation[\"image_id\"]\n",
        "    if image_id in result:\n",
        "      result[image_id].add_annotation(annotation[\"caption\"])\n",
        "    else:\n",
        "      result[image_id] = Image(image_id, [annotation[\"caption\"]])\n",
        "  for image in data[\"images\"]:\n",
        "    if image['id'] not in result:\n",
        "      continue\n",
        "    else:\n",
        "      result[image['id']].file_name = image['flickr_url']\n",
        "  return result\n",
        "\n",
        "coco_train = ImageSet(get_intermediate_dict(train))\n",
        "coco_valid = ImageSet(get_intermediate_dict(valid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCtsJMsuXk4N"
      },
      "outputs": [],
      "source": [
        "url = list(coco_train.images.values())[0].file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JH3G2CYapXh"
      },
      "outputs": [],
      "source": [
        "response = requests.get(url, stream=True)\n",
        "img = Image.open(response.raw)\n",
        "img = img.resize((224, 224))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Beo4Wzlwze"
      },
      "outputs": [],
      "source": [
        "COCO_TRAIN = \"/content/coco_train\"\n",
        "COCO_VALID = \"/content/coco_valid\"\n",
        "# load image to coco_train dir\n",
        "\n",
        "    #plt.imsave(img, f\"{path}/{image_id}.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dkiQQn1oULl"
      },
      "outputs": [],
      "source": [
        "# load training images \n",
        "load_images_to_dir(COCO_TRAIN, coco_train)\n",
        "# load validation images\n",
        "load_images_to_dir(COCO_VALID, coco_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0lu_tKukFKx"
      },
      "source": [
        "## Flickr30k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMNU8WOZkJbv"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psP200aOpJsH"
      },
      "outputs": [],
      "source": [
        "! #mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABstUAg7pfCU"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download adityajn105/flickr30k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH3UXpwHrZTK"
      },
      "outputs": [],
      "source": [
        "! unzip flickr30k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdWY85d3-2jr"
      },
      "outputs": [],
      "source": [
        "# load txt file\n",
        "flickr30k_images = {}\n",
        "with open('/content/captions.txt', 'r') as f:\n",
        "    f.readline()\n",
        "    for line in f:\n",
        "      text_data = line.split(\",\")\n",
        "      image_id = text_data[0][:-4]\n",
        "      file_name = text_data[0]\n",
        "      caption = text_data[1]\n",
        "      if image_id in flickr30k_images:\n",
        "        flickr30k_images[image_id].add_annotation(caption)\n",
        "      else:\n",
        "        flickr30k_images[image_id] = Image(image_id, [caption], file_name)\n",
        "flickr30k_dataset = ImageSet(flickr30k_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5s6LQxQj-Nv"
      },
      "source": [
        "## Google Conceptual Image Captioning Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixNXb7eZkKRy"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Important_Undergrad_Courses/csc413/googletrainingcaptions.tsv\", sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ve5z9Rl3d4d"
      },
      "outputs": [],
      "source": [
        "! mkdir google_conceptual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqj-yCborE7K"
      },
      "outputs": [],
      "source": [
        "data.columns = [\"caption\", \"image_url\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6sjTSUsrViU"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPWIIQ-tvd2N"
      },
      "outputs": [],
      "source": [
        "images = {}\n",
        "for idx, row in data.iterrows():\n",
        "  images[str(idx)] = Image(str(idx), annotations=[row[\"caption\"]], file_name=row[\"image_url\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8px_jFtG1l3-"
      },
      "outputs": [],
      "source": [
        "google_set = ImageSet(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_Vxj4ep3iSS"
      },
      "outputs": [],
      "source": [
        "GOOGLE = \"/content/google_conceptual\"\n",
        "load_images_to_dir(GOOGLE, google_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGJkQG5gB6o6"
      },
      "source": [
        "## Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFvMU4hN-FCN"
      },
      "source": [
        "What do we need to do in image preprocessing?\n",
        "- resize image (crop images)\n",
        "- smooth image a bit\n",
        "- sharpen edge a bit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtF0gqTJqOfP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPws0rjFB9pb"
      },
      "source": [
        "## Captions Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvM9aeQ5qPI2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ieac4aD35Y"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kox8J3pcD7WK"
      },
      "source": [
        "## Part 1: Object Detection -- Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8pOthb5D6ab"
      },
      "outputs": [],
      "source": [
        "## GoogLeNet\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from functorch import combine_state_for_ensemble, vmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgHY8j1pMfuF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "b9e42ce8-0930-4f54-cc4e-379c7148a3ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-eb18dba95b95>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    self.device torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "class ImageEncoder:\n",
        "  def __init__(self, choice):\n",
        "    # TODO: Define feature_extract\n",
        "\n",
        "    self.input = CROPPED_WIDTH\n",
        "    # Detect if we have a GPU available\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.trained = False\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "    self.ensemble = None\n",
        "    if choice == \"GoogLeNet\":\n",
        "      # GoogLeNet\n",
        "      self.model_name = \"GoogLeNet_Inception_v3\"\n",
        "      model_ft = models.inception_v3(pretrained=True)\n",
        "\n",
        "      # Check whether we need this: Handle the auxilary net\n",
        "      num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "      model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "      # Handle the primary net\n",
        "      num_ftrs = model_ft.fc.in_features\n",
        "      model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "      \n",
        "      self.transform = transforms.Compose([\n",
        "          transforms.Resize(224),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "      model_ft = model_ft.to(device)\n",
        "      self.model = model_ft\n",
        "\n",
        "    elif choice == \"AlexNet\":\n",
        "      self.model_name = \"AlexNet\"\n",
        "      model_ft = models.alexnet(pretrained=True)\n",
        "      self.model = model_ft.to(self.device)\n",
        "\n",
        "    #TODO\n",
        "    elif choice == \"VGG-19\":\n",
        "      self.model_name = \"VGG-19\"\n",
        "      model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)\n",
        "      self.model = model_ft.to(self.device)\n",
        "\n",
        "  def generate_ensembling_models(self, num_models):\n",
        "    # https://pytorch.org/functorch/0.1.0/notebooks/ensembling.html\n",
        "    models = [self.model for _ in range(num_models)]\n",
        "    fmodel, params, buffers = combine_state_for_ensemble(models)\n",
        "    self.ensemble = [fmodel, params, buffers]\n",
        "  \n",
        "  def train_and_val_model(self,num_epochs, trainloader, validloader, lr=0.001, momentum=0.9, loss=\"CE\"):\n",
        "    \"\"\"https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\"\"\"\n",
        "    save_file_name = f\"{self.model_name}_{num_epochs}_{lr}_{momentum}\"\n",
        "    early_stopping = 5\n",
        "    # use SGD with learning rate = 0.001 and momentum = 0.9\n",
        "    self.optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=momentum)\n",
        "    if loss==\"CE\":\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    elif loss==\"L1\":\n",
        "      criterion = nn.L1Loss()\n",
        "    else:\n",
        "      criterion = nn.MSELoss()\n",
        "    valid_loss_min = np.Inf\n",
        "    epochs_no_improve, valid_best_acc = 0, 0\n",
        "    outputs_losses = []\n",
        "    losses_colums = [\"training_loss\",\"validation_loss\"]\n",
        "    outputs_acc = []\n",
        "    acc_columns = [\"training_acc\",\"validation_acc\"]\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      training_loss, validation_loss, testing_loss = 0, 0, 0\n",
        "      training_acc, validation_acc, testing_acc = 0, 0, 0\n",
        "      for images, labels in trainloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        training_loss += loss.item() * images.size(0)\n",
        "        # Get training accuracy\n",
        "        _, pred = torch.max(output, dim=1)\n",
        "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "        accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "        training_acc += accuracy.item() * images.size(0)\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "          self.model.eval()\n",
        "          print(\"model start validating\")\n",
        "          for images, labels in validloader:\n",
        "            images = images.view(images.shape[0], -1)\n",
        "            output = self.model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            validation_loss += loss.item() * images.size(0)\n",
        "            # Get Validation accuracy\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "            accuracy = torch.mean(\n",
        "                correct_tensor.type(torch.FloatTensor))\n",
        "            validation_acc += accuracy.item() * images.size(0)\n",
        "       # Calculate average losses\n",
        "        train_loss = training_loss / len(trainloader.dataset)\n",
        "        valid_loss = validation_loss / len(validloader.dataset)\n",
        "        # Calculate average accuracy\n",
        "        train_acc = training_acc / len(trainloader.dataset)\n",
        "        valid_acc = validation_acc / len(validloader.dataset)\n",
        "        outputs_losses.append([\n",
        "            train_loss, valid_loss\n",
        "        ])\n",
        "        outputs_acc.append([\n",
        "            train_acc, valid_acc\n",
        "        ])\n",
        "        # Save the model if validation loss decreases\n",
        "        if valid_loss < valid_loss_min:\n",
        "            # Save model\n",
        "            torch.save(self.model.state_dict(), save_file_name)\n",
        "            # Track improvement\n",
        "            epochs_no_improve = 0\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= early_stopping:\n",
        "                print(\"Early stopping the model.\")\n",
        "                self.model.load_state_dict(torch.load(save_file_name))\n",
        "                self.model.optimizer = self.optimizer\n",
        "                outputs_losses = pd.DataFrame(\n",
        "                    outputs_losses,\n",
        "                    columns=losses_colums\n",
        "                )\n",
        "                outputs_acc = pd.DataFrame(\n",
        "                    outputs_acc,\n",
        "                    columns=acc_columns\n",
        "                )\n",
        "                # self.model = model\n",
        "                self.trained = True\n",
        "                return self.model, outputs_losses, outputs_acc\n",
        "    outputs_losses = pd.DataFrame(\n",
        "        outputs_losses,\n",
        "        columns=losses_colums\n",
        "    )\n",
        "    outputs_acc = pd.DataFrame(\n",
        "        outputs_acc,\n",
        "        columns=acc_columns\n",
        "    )\n",
        "    # self.model = model\n",
        "    self.trained = True\n",
        "    return self.model, outputs_losses, outputs_acc\n",
        "  \n",
        "  def forward(self, images, num_models, dropout=False, ensemble=False):\n",
        "    # think about the ensemble and dropout a bit more\n",
        "    if ensemble and self.ensemble == None:\n",
        "      self.generate_ensembling_models(num_models)\n",
        "    if dropout and ensemble:\n",
        "      minibatches = images[:num_models]\n",
        "      model_outputs = self.dropout(vmap(self.ensemble[0])(self.ensemble[1], self.ensemble[2], minibatches))\n",
        "    elif dropout:\n",
        "      model_outputs = self.dropout(self.model(images))\n",
        "    elif ensemble:\n",
        "      minibatches = images[:num_models]\n",
        "      model_outputs = vmap(self.ensemble[0])(self.ensemble[1], self.ensemble[2], minibatches)\n",
        "    else:\n",
        "      model_outputs = self.model(images)\n",
        "    # might not want embeddings?\n",
        "    #embeddings = self.embed(model_outputs)\n",
        "    return model_outputs\n",
        "\n",
        "  def encode_image(self, image_path, normalize=False):\n",
        "    \"\"\"\n",
        "    have to make this so that this can also process a lot of images at the same time\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    img_processed = self.transform(img).unsqueeze(0)\n",
        "    if torch.cuda.is_available():\n",
        "      img_processed = img_processed.to('cuda')\n",
        "      self.model.to('cuda')\n",
        "\n",
        "    if not self.trained:\n",
        "      print(\"Model has not been trained\")\n",
        "      return -1\n",
        "    with torch.no_grad():\n",
        "      output = self.model(img_processed)[0]\n",
        "    if normalize:\n",
        "      # add normalization code here\n",
        "      output = torch.nn.functional.softmax(output, dim=0)\n",
        "    return output\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i64RpSkFKUFI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp0pc5TXQzwQ"
      },
      "source": [
        "## Part 2 Attention Mechanism Extracting high level features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9gImq5FQ-t_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LULo91ppQ_OD"
      },
      "source": [
        "## Part 3 Sentence Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukjqtu_0mu_n"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "class AnnotationCleaner:\n",
        "  def removePunctuation(self, annotation):\n",
        "    # remove the punctuations\n",
        "    return re.sub(r'[^\\w\\s]', '', test_str)\n",
        "\n",
        "  def lower(self, annotation):\n",
        "    # turn captions to lower\n",
        "    return annotation.lower()\n",
        "  \n",
        "  def removeExtraSpace(self, annotation):\n",
        "    # remove extra spaces\n",
        "    new = re.sub(r\" {2,}\", \" \", annotation)\n",
        "    return new.strip()\n",
        "\n",
        "  def __call__(self, annotation):\n",
        "    output = self.removePunctuation(annotation)\n",
        "    output = self.lower(annotation)\n",
        "    output = self.removeExtraSpace(annotation)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxpjNUTKRGxu"
      },
      "outputs": [],
      "source": [
        "class Vocabularies:\n",
        "  def __init__(self):\n",
        "    self.word_to_idx = {}\n",
        "    self.idx_to_word = {}\n",
        "    self.len = 0\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word_to_idx:\n",
        "      self.word_to_idx[word] = self.len + 1\n",
        "      self.len += 1\n",
        "      self.idx_to_word[self.len] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfPfsSFA2JPF"
      },
      "source": [
        "We can use index to represent the words, but we can also use glove for the embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvqA1Wy14iiR"
      },
      "outputs": [],
      "source": [
        "# Build list of vocabs \n",
        "threshold = 4\n",
        "def build_giant_list_of_vocabs(imageset):\n",
        "  words = {}\n",
        "  words_count = {}\n",
        "  vocabs = Vocabularies()\n",
        "  for img_id, image in imageset.images.items():\n",
        "    for caption in image.annotations:\n",
        "      for word in caption.split():\n",
        "        if word not in words_count:\n",
        "          words_count[word] = 1\n",
        "        else:\n",
        "          words_count[word] += 1\n",
        "        if words_count[word] > threshold:\n",
        "          words.add(word)\n",
        "          vocab.add_word(word)\n",
        "  # return a set of words, so that there would not be duplicates\n",
        "  return words, words_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yvsNiZlmbReM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download glove\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n",
        "!git clone https://github.com/stanfordnlp/glove\n",
        "!cd glove && make\n",
        "\n",
        "!pip install glove_python"
      ],
      "metadata": {
        "id": "YDU5vPjHb8gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate N-Grams\n",
        "def n_grams(seq:Sequence[str], n:int) -> List:\n",
        "    '''Extract all n-grams from a sequence\n",
        "\n",
        "    An n-gram is a contiguous sub-sequence within `seq` of length `n`. This\n",
        "    function extracts them (in order) from `seq`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seq : sequence\n",
        "        A sequence of words or token ids representing a transcription.\n",
        "    n : int\n",
        "        The size of sub-sequence to extract.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ngrams : list\n",
        "    '''\n",
        "\n",
        "    #Gets all n-grams in refernece input IN ORDER\n",
        "    #Ex ref=friendship is magic\n",
        "    #This returns [[friendship, is][is, magic]]\n",
        "\n",
        "    total = []\n",
        "    for sentence in seq: \n",
        "      i = 0\n",
        "      ngrams = []\n",
        "      while i+n <= len(sentence):\n",
        "          ngrams.append(sentence[i:i+n])\n",
        "          i+=1\n",
        "      total.append(ngrams)\n",
        "    return total\n"
      ],
      "metadata": {
        "id": "obr3gt68gkud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert Tokenizer\n",
        "! pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "def tokenize(data): \n",
        "  tokenized_inputs = bert_tokenizer(\n",
        "      data,          # Input text\n",
        "      add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n",
        "      padding='max_length',     # pad to a length specified by the max_length\n",
        "      max_length=280,       # truncate all sentences longer than max_length -- max # of characters in Twitter\n",
        "      return_tensors='pt',      # return everything we need as PyTorch tensors\n",
        "  )\n",
        "\n",
        "  input_ids = tokenized_inputs['input_ids']\n",
        "  attention_masks = tokenized_inputs['attention_mask']\n",
        "  return input_ids, attention_masks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU6JNbe4h2Ht",
        "outputId": "aad3ed88-1e1d-4206-d6ab-f3ab06b22f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.0 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(data['vocab']) # Number of vocabs\n",
        "#from 413 PA1\n",
        "def calculate_log_co_occurence(word_data, symmetric=False):\n",
        "  \"Compute the log-co-occurence matrix for our data.\"\n",
        "  log_co_occurence = np.zeros((vocab_size, vocab_size))\n",
        "  for input in word_data:\n",
        "    # Note: the co-occurence matrix may not be symmetric\n",
        "    log_co_occurence[input[0], input[1]] += 1\n",
        "    log_co_occurence[input[1], input[2]] += 1\n",
        "    log_co_occurence[input[2], input[3]] += 1\n",
        "    # Diagonal entries are just the frequency of the word\n",
        "    log_co_occurence[input[0], input[0]] += 1\n",
        "    log_co_occurence[input[1], input[1]] += 1\n",
        "    log_co_occurence[input[2], input[2]] += 1\n",
        "    log_co_occurence[input[3], input[3]] += 1\n",
        "    # If we want symmetric co-occurence can also increment for these.\n",
        "    if symmetric:\n",
        "      log_co_occurence[input[1], input[0]] += 1\n",
        "      log_co_occurence[input[2], input[1]] += 1\n",
        "      log_co_occurence[input[3], input[2]] += 1\n",
        "  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n",
        "  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n",
        "  log_co_occurence = np.log(log_co_occurence)\n",
        "  return log_co_occurence\n",
        "\n",
        "def loss_GloVe(W, W_tilde, b, b_tilde, log_co_occurence):\n",
        "  \"\"\" Compute the GloVe loss given the parameters of the model. When W_tilde \n",
        "  and b_tilde are not given, then the model is symmetric (i.e. W_tilde = W,\n",
        "  b_tilde = b). \n",
        "\n",
        "  Args:\n",
        "    W: word embedding matrix, dimension V x d where V is vocab size and d\n",
        "      is the embedding dimension\n",
        "    W_tilde: for asymmetric GloVe model, a second word embedding matrix, with\n",
        "      dimensions V x d\n",
        "    b: bias vector, dimension V.\n",
        "    b_tilde: for asymmetric GloVe model, a second bias vector, dimension V\n",
        "    log_co_occurence: V x V log co-occurrence matrix (log X)\n",
        "  \n",
        "  Returns:\n",
        "    loss: a scalar (float) for GloVe loss\n",
        "  \"\"\"\n",
        "  n,_ = log_co_occurence.shape\n",
        "  # Symmetric Case, no W_tilde and b_tilde\n",
        "  if W_tilde is None and b_tilde is None:\n",
        "    # Symmetric model\n",
        "    ###########################   YOUR CODE HERE  ##############################\n",
        "    A = np.matmul(W, W.T) + b*1 + b*1 - log_co_occurence\n",
        "    loss = np.trace(np.matmul(A.T, A))\n",
        "    ############################################################################\n",
        "  else: \n",
        "    # Asymmetric model\n",
        "    ###########################   YOUR CODE HERE  ##############################\n",
        "    A = np.matmul(W, W_tilde.T) + b*1 + b_tilde*1 - log_co_occurence\n",
        "    loss = np.trace(np.matmul(A.T, A))\n",
        "    ############################################################################\n",
        "  return loss\n",
        "\n",
        "def grad_GloVe(W, W_tilde, b, b_tilde, log_co_occurence):\n",
        "  \"\"\"Return the gradient of GloVe objective w.r.t its parameters\n",
        "  Args:\n",
        "    W: word embedding matrix, dimension V x d where V is vocab size and d\n",
        "      is the embedding dimension\n",
        "    W_tilde: for asymmetric GloVe model, a second word embedding matrix, with\n",
        "      dimensions V x d\n",
        "    b: bias vector, dimension V.\n",
        "    b_tilde: for asymmetric GloVe model, a second bias vector, dimension V\n",
        "    log_co_occurence: V x V log co-occurrence matrix (log X)\n",
        "  \n",
        "  Returns:\n",
        "    grad_W: gradient of the loss wrt W, dimension V x d\n",
        "    grad_W_tilde: gradient of the loss wrt W_tilde, dimension V x d. Return \n",
        "      None if W_tilde is None.\n",
        "    grad_b: gradient of the loss wrt b, dimension V x 1\n",
        "    grad_b_tilde: gradient of the loss wrt b, dimension V x 1. Return \n",
        "      None if b_tilde is None.\n",
        "  \"\"\"\n",
        "  n,_ = log_co_occurence.shape\n",
        "  \n",
        "  if W_tilde is None and b_tilde is None:\n",
        "    # Symmmetric case\n",
        "    ###########################   YOUR CODE HERE  ##############################\n",
        "    A = np.matmul(W, W.T) + b*1 + b*1 - log_co_occurence\n",
        "    grad_W =  2*np.matmul(A, W) + 2*np.matmul(A.T, W) \n",
        "    grad_b = 4*(np.matmul(A, np.ones(b.shape)))\n",
        "    grad_W_tilde = None\n",
        "    grad_b_tilde = None\n",
        "    ############################################################################\n",
        "  else:\n",
        "    # Asymmetric case\n",
        "    ###########################   YOUR CODE HERE  ##############################\n",
        "    A = np.matmul(W, W_tilde.T) + b*1 + b_tilde*1 - log_co_occurence\n",
        "    grad_W = 2*np.matmul(A, W_tilde)\n",
        "    grad_W_tilde = 2*np.matmul(A.T, W)\n",
        "    grad_b = 2*(np.matmul(A, np.ones(b.shape)))\n",
        "    grad_b_tilde = 2*(np.matmul(A, np.ones(b_tilde.shape)))\n",
        "    ############################################################################\n",
        "    \n",
        "  return grad_W, grad_W_tilde, grad_b, grad_b_tilde\n",
        "\n",
        "def train_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n",
        "  \"Traing W and b according to GloVe objective.\"\n",
        "  n,_ = log_co_occurence_train.shape\n",
        "  learning_rate = 0.05 / n  # A hyperparameter.  You can play with this if you want.\n",
        "  train_loss_list = np.zeros(n_epochs)\n",
        "  valid_loss_list = np.zeros(n_epochs)\n",
        "  vocab_size = log_co_occurence_train.shape[0]\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    grad_W, grad_W_tilde, grad_b, grad_b_tilde = grad_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_train)\n",
        "    W = W - learning_rate * grad_W\n",
        "    b = b - learning_rate * grad_b\n",
        "    if not grad_W_tilde is None and not grad_b_tilde is None:\n",
        "      W_tilde = W_tilde - learning_rate * grad_W_tilde\n",
        "      b_tilde = b_tilde - learning_rate * grad_b_tilde\n",
        "    train_loss, valid_loss = loss_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_train), loss_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_valid)\n",
        "    if do_print:\n",
        "      print(f\"Average Train Loss: {train_loss / vocab_size}, Average valid loss: {valid_loss / vocab_size}, grad_norm: {np.sum(grad_W**2)}\")\n",
        "    train_loss_list[epoch] = train_loss / vocab_size\n",
        "    valid_loss_list[epoch] = valid_loss / vocab_size\n",
        "\n",
        "  return W, W_tilde, b, b_tilde, train_loss_list, valid_loss_list\n",
        "\n",
        "def generate_glove_embeddings(training, validation):\n",
        "  \"\"\"\n",
        "  training: matrix of words\n",
        "  validation: matrix of words\n",
        "  \"\"\"\n",
        "  train_tokenize = tokenize(training)\n",
        "  valid_tokenize = tokenize(validation)\n",
        "  train4 = n_grams(train_tokenize, 4)\n",
        "  valid4 = n_grams(valid_tokenize,4)\n",
        "  train_co = log_co_occurence(train4)\n",
        "  valid_co = log_co_occurence(valid4)\n",
        "\n",
        "  init_variance = 0.05  # A hyperparameter.  You can play with this if you want.\n",
        "  embedding_dim = 16\n",
        "  W = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "  W_tilde = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "  b = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "  b_tilde = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "\n",
        "  return train_GloVe(W, W_tilde, b, b_tilde, train_co, valid_co, \n",
        "                     n_epochs=3, do_print=True)\n"
      ],
      "metadata": {
        "id": "zhRieEEBb_xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training + Helper functions for NMT Training\n",
        "\n",
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
        "    return [char_to_index[char] for char in s] + [\n",
        "        end_token\n",
        "    ]  # Adds the end token to each index list\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if isinstance(attention_weights, tuple):\n",
        "        ## transformer's attention mweights\n",
        "        attention_weights, self_attention_weights = attention_weights\n",
        "\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "\n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
        "        ax.set_xticklabels(\n",
        "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
        "        )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = (\n",
        "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
        "            )  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat(\n",
        "                [decoder_input, targets[:, 0:-1]], dim=1\n",
        "            )  # Gets decoder inputs by shifting the targets to the right\n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(\n",
        "                decoder_inputs, encoder_annotations, decoder_hidden\n",
        "            )\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss = compute_loss(\n",
        "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss = compute_loss(\n",
        "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
        "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
        "            )\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Data Stats\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
        "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = GRUEncoder(\n",
        "            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n",
        "        )\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "            opts=opts,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      encoder = AttentionEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            opts=opts,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == \"rnn\":\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == \"rnn_attention\":\n",
        "        decoder = RNNAttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            attention_type=opts.attention_type,\n",
        "        )\n",
        "    elif opts.decoder_type == \"transformer\":\n",
        "        decoder = TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      decoder = AttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #### setup checkpoint path\n",
        "    model_name = \"h{}-bs{}-{}-{}\".format(\n",
        "        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n",
        "    )\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return encoder, decoder, losses\n",
        "\n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Opts\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "1DWrlUlxwG3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run Training \n",
        "\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "ROXbU5dswf0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Wix = nn.Linear(input_size, hidden_size)\n",
        "        self.Wim = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wfx = nn.Linear(input_size, hidden_size)\n",
        "        self.Wfm = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wox = nn.Linear(input_size, hidden_size)\n",
        "        self.Wom = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wcx = nn.Linear(input_size, hidden_size)\n",
        "        self.Wcm = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, m_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "        i_t = torch.sigmoid(self.Wix(x) + self.Wim(m_prev))\n",
        "        f_t = torch.sigmoid(self.Wfx(x) + self.Wfm(m_prev))\n",
        "        o_t = torch.sigmoid(self.Wox(x) + self.Wom(m_prev))\n",
        "        c_new = f_t * c_prev + i_t * torch.tanh(self.Wcx(x) + self.Wcm(m_prev))\n",
        "        m_new = o_t * c_new\n",
        "        p_new = torch.softmax(m_new)\n",
        "        return m_new, c_new, p_new"
      ],
      "metadata": {
        "id": "h6iniLG3vevZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "_FNM1lpQAOHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNNCell(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_size):\n",
        "        \"\"\"Initialize RNN Cell.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "\n",
        "        # self.relu = nn.ReLU()\n",
        "        # self.Ww = nn.Linear(obs_dim, obs_dim) # TODO: Check this\n",
        "\n",
        "        # self.We = nn.Linear(obs_dim, hidden_size)\n",
        "\n",
        "        # self.Wf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # self.Wb = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # self.Wd = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Whi = 1 # Last layer of CNN\n",
        "        self.Whx = nn.Linear(obs_dim, hidden_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Woh = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x, h_f, h_b, CNN_last_layer):\n",
        "        \"\"\"Compute forward pass for this RNN cell.\"\"\"\n",
        "        # x_t = self.Ww() # Some identity?\n",
        "\n",
        "        # e_t = self.relu(self.We(x_t))\n",
        "\n",
        "        # h_ft = self.relu(e_t + self.Wf(h_f))\n",
        "\n",
        "        # h_bt = self.relu(e_t + self.Wb(h_b))\n",
        "\n",
        "        # s_t = self.relu(self.Wd(h_ft + h_bt))\n",
        "\n",
        "        b_v = self.Whi(CNN_last_layer)\n",
        "        h_t = self.relu(self.Whx(x) + self.Whh(h_t_prev) + ) # TODO\n",
        "        y_t = self.softmax(self.Woh(h_t) + )\n",
        "\n",
        "        return s_t\n",
        "    \n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_size, output_dim):\n",
        "        \"\"\"Initialize RNN.\"\"\"\n",
        "        super().__init__()\n",
        "        self.obs_dim = obs_dim\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.rnn_cell = MyRNNCell(obs_dim, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Compute forward pass on sequence x.\n",
        "        \n",
        "        Input sequence x has shape (B x L x D), where:\n",
        "        B is batch size, L is sequence length, and D is the number of features.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, n_feat = x.size()\n",
        "        \n",
        "        # Stores outputs of RNN cell\n",
        "        output_arr = torch.zeros((batch_size, seq_len, self.output_dim))\n",
        "        hidden_arr = torch.zeros((batch_size, seq_len, self.hidden_size))\n",
        "        \n",
        "        # Send to GPU. This is a gotcha, make sure to send Tensors created\n",
        "        # in a model to the same device as input Tensors.\n",
        "        output_arr = output_arr.float().to(x.device)\n",
        "        hidden_arr = hidden_arr.float().to(x.device)\n",
        "\n",
        "        hidden = self.init_hidden(batch_size, x.device)\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            # For each iteration, compute RNN on input for current position\n",
        "            output, hidden = self.rnn_cell(x[:, i, :], hidden)\n",
        "\n",
        "            output_arr[:, i, :] = output\n",
        "            hidden_arr[:, i, :] = hidden\n",
        "\n",
        "        return output_arr, hidden_arr\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        \"\"\"Initialize RNN hidden state.\n",
        "        \n",
        "        Some people advocate for using random noise instead of zeros, or \n",
        "        training for the initial state. Personally, I don't know if it matters!\n",
        "        \"\"\"\n",
        "        return torch.zeros(batch_size, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "aCGkEZAcov_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMVhnFIw8bwk"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpqAR8yN8fYm"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXHh7lf8j6Z"
      },
      "outputs": [],
      "source": [
        "# Example Usage of sentence_bleu\n",
        "ref = [\n",
        "    'this is moonlight'.split(),\n",
        "    'Look, this is moonlight'.split(),\n",
        "    'moonlight it is'.split()\n",
        "]\n",
        "test = 'it is moonlight'.split()\n",
        "print('BLEU score for test-> {}'.format(sentence_bleu(ref, test)))\n",
        " \n",
        "test01 = 'it is cat and moonlight'.split()\n",
        "print('BLEU score for test01-> {}'.format(sentence_bleu(ref, test01)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c0lu_tKukFKx",
        "Q5s6LQxQj-Nv",
        "SGJkQG5gB6o6"
      ],
      "name": "CSC413_Playground.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}